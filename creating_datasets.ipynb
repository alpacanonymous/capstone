{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"font-size:2em;\"> Creating Datasets for Four Major Air Pollutants 2000 - 2021\n",
    "Author: Angela Kim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"font-size:1.2em;\"> Contents\n",
    "<l></l>\n",
    "\n",
    "<span style=\"font-size:1.2em;\">\n",
    "\n",
    "- <a href=\"#Overview\">Overview</a>\n",
    "    \n",
    "- <a href=\"#Creating Individual Pollutant Datasets\">Creating Individual Pollutant Datasets</a>\n",
    "    \n",
    "    - <a href=\"#Ozone\">Ozone</a>\n",
    "    \n",
    "    - <a href=\"#Carbon Monoxide\">Carbon Monoxide</a>\n",
    "    \n",
    "    - <a href=\"#Nitrogen Dioxide\">Nitrogen Dioxide</a>\n",
    "\n",
    "    - <a href=\"#Sulfur Dioxide\">Sulfur Dioxide</a>\n",
    "    \n",
    "- <a href=\"#Creating Trim Pollutant Datasets\">Creating Trim Pollutant Datasets</a>\n",
    "\n",
    "- <a href=\"#Creating Time Series Datasets\">Creating Time Series Datasets</a>\n",
    "    \n",
    "- <a href=\"#Export\">Export</a>\n",
    "\n",
    "- <a href=\"#Sources\">Sources</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"font-size:1.2em;\"> <a id=\"Overview\">Overview</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project, I originally planned on using a dataset from [Kaggle](https://www.kaggle.com/sogun3/uspollution) on US Air Quality from 2000-2016 and then tacking on more recent data from 2017-2021. However, as I began to explore the data, I was incredibly frustrated with how poorly the data had been put together. I did my best to clean it while maintaining the integrity of the data but ultimately decided that it would probably end up being less frustrating, less time-consuming, and much cleaner if I started at the source.\n",
    "\n",
    "The [US EPA](https://www.epa.gov/) has open-source pre-generated data files in `.csv` format on the four major air pollutants I'm interested in for this project: ozone ($O_{3}$), carbon monoxide ($CO$), nitrogen dioxide ($NO_{2}$), and sulfur dioxide ($SO_{2}$).\n",
    "\n",
    "First, I downloaded a total of 88 `.csv` [daily summary data files](https://aqs.epa.gov/aqsweb/airdata/download_files.html#Daily) for the years 2000-2021 and concatenated them into their respective pollutant datasets. Second, I took those and trimmed them down. I kept data only of the 50 US States and DC, dropped rows where the `Pollutant Standard` did not produce `AQI` values, got rid of columns that were either redundant or unnecessary for the purposes of this project, and renamed a few columns for conciseness. Third, I took those and created time series datasets that only contained `Date`, `State`, `County`, `City`, and `AQI`. Fourth, I extracted the data pertaining only to NYC to start modeling on a smaller scale (with hopes I had enough time to expand nationwide) and made four more datasets. Finally, I exported all 16 datasets as `.csv` files.\n",
    "\n",
    "Many of the resulting `.csv` files were too large to upload onto github with its limit of 100MB, but you can download all the files I used from the EPA site and run this notebook to get the compiled datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Please consider:**\n",
    "1. Take note of where you stored the downloaded data and adjusting the code accordingly before running the notebook.\n",
    "2. Make sure to uncomment the export lines if you want the datasets as `.csv`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"font-size:1.2em;\"> <a id=\"Creating Individual Pollutant Datasets\">Creating Individual Pollutant Datasets</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T15:17:51.833795Z",
     "start_time": "2022-01-25T15:17:51.251032Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"font-size:1.2em;\"> <a id=\"Ozone\">Ozone</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T15:18:32.652808Z",
     "start_time": "2022-01-25T15:17:51.836559Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import Ozone datasets\n",
    "O3_2000 = pd.read_csv('data/dailyO3/daily_44201_2000.csv')\n",
    "O3_2001 = pd.read_csv('data/dailyO3/daily_44201_2001.csv')\n",
    "O3_2002 = pd.read_csv('data/dailyO3/daily_44201_2002.csv')\n",
    "O3_2003 = pd.read_csv('data/dailyO3/daily_44201_2003.csv')\n",
    "O3_2004 = pd.read_csv('data/dailyO3/daily_44201_2004.csv')\n",
    "O3_2005 = pd.read_csv('data/dailyO3/daily_44201_2005.csv')\n",
    "O3_2006 = pd.read_csv('data/dailyO3/daily_44201_2006.csv')\n",
    "O3_2007 = pd.read_csv('data/dailyO3/daily_44201_2007.csv')\n",
    "O3_2008 = pd.read_csv('data/dailyO3/daily_44201_2008.csv')\n",
    "O3_2009 = pd.read_csv('data/dailyO3/daily_44201_2009.csv')\n",
    "O3_2010 = pd.read_csv('data/dailyO3/daily_44201_2010.csv')\n",
    "O3_2011 = pd.read_csv('data/dailyO3/daily_44201_2011.csv')\n",
    "O3_2012 = pd.read_csv('data/dailyO3/daily_44201_2012.csv')\n",
    "O3_2013 = pd.read_csv('data/dailyO3/daily_44201_2013.csv')\n",
    "O3_2014 = pd.read_csv('data/dailyO3/daily_44201_2014.csv')\n",
    "O3_2015 = pd.read_csv('data/dailyO3/daily_44201_2015.csv')\n",
    "O3_2016 = pd.read_csv('data/dailyO3/daily_44201_2016.csv')\n",
    "O3_2017 = pd.read_csv('data/dailyO3/daily_44201_2017.csv')\n",
    "O3_2018 = pd.read_csv('data/dailyO3/daily_44201_2018.csv')\n",
    "O3_2019 = pd.read_csv('data/dailyO3/daily_44201_2019.csv')\n",
    "O3_2020 = pd.read_csv('data/dailyO3/daily_44201_2020.csv')\n",
    "O3_2021 = pd.read_csv('data/dailyO3/daily_44201_2021.csv')\n",
    "\n",
    "\n",
    "# Concatenate datasets\n",
    "O3_all = [O3_2000, O3_2001, O3_2002, O3_2003, O3_2004, O3_2005, O3_2006, O3_2007, O3_2008, O3_2009, O3_2010, \n",
    "          O3_2011, O3_2012, O3_2013, O3_2014, O3_2015, O3_2016, O3_2017, O3_2018, O3_2019, O3_2020, O3_2021]\n",
    "\n",
    "O3 = pd.concat(O3_all, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-18T22:12:43.711053Z",
     "start_time": "2022-01-18T22:10:24.802435Z"
    }
   },
   "source": [
    "## <span style=\"font-size:1.2em;\"> <a id=\"Carbon Monoxide\">Carbon Monoxide</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T15:19:02.957001Z",
     "start_time": "2022-01-25T15:18:32.659163Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import Carbon Monoxide datasets\n",
    "CO_2000 = pd.read_csv('data/dailyCO/daily_42101_2000.csv')\n",
    "CO_2001 = pd.read_csv('data/dailyCO/daily_42101_2001.csv')\n",
    "CO_2002 = pd.read_csv('data/dailyCO/daily_42101_2002.csv')\n",
    "CO_2003 = pd.read_csv('data/dailyCO/daily_42101_2003.csv')\n",
    "CO_2004 = pd.read_csv('data/dailyCO/daily_42101_2004.csv')\n",
    "CO_2005 = pd.read_csv('data/dailyCO/daily_42101_2005.csv')\n",
    "CO_2006 = pd.read_csv('data/dailyCO/daily_42101_2006.csv')\n",
    "CO_2007 = pd.read_csv('data/dailyCO/daily_42101_2007.csv')\n",
    "CO_2008 = pd.read_csv('data/dailyCO/daily_42101_2008.csv')\n",
    "CO_2009 = pd.read_csv('data/dailyCO/daily_42101_2009.csv')\n",
    "CO_2010 = pd.read_csv('data/dailyCO/daily_42101_2010.csv')\n",
    "CO_2011 = pd.read_csv('data/dailyCO/daily_42101_2011.csv')\n",
    "CO_2012 = pd.read_csv('data/dailyCO/daily_42101_2012.csv')\n",
    "CO_2013 = pd.read_csv('data/dailyCO/daily_42101_2013.csv')\n",
    "CO_2014 = pd.read_csv('data/dailyCO/daily_42101_2014.csv')\n",
    "CO_2015 = pd.read_csv('data/dailyCO/daily_42101_2015.csv')\n",
    "CO_2016 = pd.read_csv('data/dailyCO/daily_42101_2016.csv')\n",
    "CO_2017 = pd.read_csv('data/dailyCO/daily_42101_2017.csv')\n",
    "CO_2018 = pd.read_csv('data/dailyCO/daily_42101_2018.csv')\n",
    "CO_2019 = pd.read_csv('data/dailyCO/daily_42101_2019.csv')\n",
    "CO_2020 = pd.read_csv('data/dailyCO/daily_42101_2020.csv')\n",
    "CO_2021 = pd.read_csv('data/dailyCO/daily_42101_2021.csv')\n",
    "\n",
    "\n",
    "# Concatenate datasets\n",
    "CO_all = [CO_2000, CO_2001, CO_2002, CO_2003, CO_2004, CO_2005, CO_2006, CO_2007, CO_2008, CO_2009, CO_2010,\n",
    "          CO_2011, CO_2012, CO_2013, CO_2014, CO_2015, CO_2016, CO_2017, CO_2018, CO_2019, CO_2020, CO_2021]\n",
    "\n",
    "CO = pd.concat(CO_all, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"font-size:1.2em;\"> <a id=\"Nitrogen Dioxide\">Nitrogen Dioxide</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T15:19:20.760029Z",
     "start_time": "2022-01-25T15:19:02.975686Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import Nitrogen Dioxide datasets\n",
    "NO2_2000 = pd.read_csv('data/dailyNO2/daily_42602_2000.csv')\n",
    "NO2_2001 = pd.read_csv('data/dailyNO2/daily_42602_2001.csv')\n",
    "NO2_2002 = pd.read_csv('data/dailyNO2/daily_42602_2002.csv')\n",
    "NO2_2003 = pd.read_csv('data/dailyNO2/daily_42602_2003.csv')\n",
    "NO2_2004 = pd.read_csv('data/dailyNO2/daily_42602_2004.csv')\n",
    "NO2_2005 = pd.read_csv('data/dailyNO2/daily_42602_2005.csv')\n",
    "NO2_2006 = pd.read_csv('data/dailyNO2/daily_42602_2006.csv')\n",
    "NO2_2007 = pd.read_csv('data/dailyNO2/daily_42602_2007.csv')\n",
    "NO2_2008 = pd.read_csv('data/dailyNO2/daily_42602_2008.csv')\n",
    "NO2_2009 = pd.read_csv('data/dailyNO2/daily_42602_2009.csv')\n",
    "NO2_2010 = pd.read_csv('data/dailyNO2/daily_42602_2010.csv')\n",
    "NO2_2011 = pd.read_csv('data/dailyNO2/daily_42602_2011.csv')\n",
    "NO2_2012 = pd.read_csv('data/dailyNO2/daily_42602_2012.csv')\n",
    "NO2_2013 = pd.read_csv('data/dailyNO2/daily_42602_2013.csv')\n",
    "NO2_2014 = pd.read_csv('data/dailyNO2/daily_42602_2014.csv')\n",
    "NO2_2015 = pd.read_csv('data/dailyNO2/daily_42602_2015.csv')\n",
    "NO2_2016 = pd.read_csv('data/dailyNO2/daily_42602_2016.csv')\n",
    "NO2_2017 = pd.read_csv('data/dailyNO2/daily_42602_2017.csv')\n",
    "NO2_2018 = pd.read_csv('data/dailyNO2/daily_42602_2018.csv')\n",
    "NO2_2019 = pd.read_csv('data/dailyNO2/daily_42602_2019.csv')\n",
    "NO2_2020 = pd.read_csv('data/dailyNO2/daily_42602_2020.csv')\n",
    "NO2_2021 = pd.read_csv('data/dailyNO2/daily_42602_2021.csv')\n",
    "\n",
    "\n",
    "# Concatenate datasets\n",
    "NO2_all = [NO2_2000, NO2_2001, NO2_2002, NO2_2003, NO2_2004, NO2_2005, NO2_2006, NO2_2007, NO2_2008, NO2_2009, \n",
    "           NO2_2010, NO2_2011, NO2_2012, NO2_2013, NO2_2014, NO2_2015, NO2_2016, NO2_2017, NO2_2018, NO2_2019, \n",
    "           NO2_2020, NO2_2021]\n",
    "\n",
    "NO2 = pd.concat(NO2_all, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"font-size:1.2em;\"> <a id=\"Sulfur Dioxide\">Sulfur Dioxide</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T15:20:02.925101Z",
     "start_time": "2022-01-25T15:19:20.763231Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import Sulfur Dioxide datasets\n",
    "SO2_2000 = pd.read_csv('data/dailySO2/daily_42401_2000.csv')\n",
    "SO2_2001 = pd.read_csv('data/dailySO2/daily_42401_2001.csv')\n",
    "SO2_2002 = pd.read_csv('data/dailySO2/daily_42401_2002.csv')\n",
    "SO2_2003 = pd.read_csv('data/dailySO2/daily_42401_2003.csv')\n",
    "SO2_2004 = pd.read_csv('data/dailySO2/daily_42401_2004.csv')\n",
    "SO2_2005 = pd.read_csv('data/dailySO2/daily_42401_2005.csv')\n",
    "SO2_2006 = pd.read_csv('data/dailySO2/daily_42401_2006.csv')\n",
    "SO2_2007 = pd.read_csv('data/dailySO2/daily_42401_2007.csv')\n",
    "SO2_2008 = pd.read_csv('data/dailySO2/daily_42401_2008.csv')\n",
    "SO2_2009 = pd.read_csv('data/dailySO2/daily_42401_2009.csv')\n",
    "SO2_2010 = pd.read_csv('data/dailySO2/daily_42401_2010.csv')\n",
    "SO2_2011 = pd.read_csv('data/dailySO2/daily_42401_2011.csv')\n",
    "SO2_2012 = pd.read_csv('data/dailySO2/daily_42401_2012.csv')\n",
    "SO2_2013 = pd.read_csv('data/dailySO2/daily_42401_2013.csv')\n",
    "SO2_2014 = pd.read_csv('data/dailySO2/daily_42401_2014.csv')\n",
    "SO2_2015 = pd.read_csv('data/dailySO2/daily_42401_2015.csv')\n",
    "SO2_2016 = pd.read_csv('data/dailySO2/daily_42401_2016.csv')\n",
    "SO2_2017 = pd.read_csv('data/dailySO2/daily_42401_2017.csv')\n",
    "SO2_2018 = pd.read_csv('data/dailySO2/daily_42401_2018.csv')\n",
    "SO2_2019 = pd.read_csv('data/dailySO2/daily_42401_2019.csv')\n",
    "SO2_2020 = pd.read_csv('data/dailySO2/daily_42401_2020.csv')\n",
    "SO2_2021 = pd.read_csv('data/dailySO2/daily_42401_2021.csv')\n",
    "\n",
    "\n",
    "# Concatenate datasets\n",
    "SO2_all = [SO2_2000, SO2_2001, SO2_2002, SO2_2003, SO2_2004, SO2_2005, SO2_2006, SO2_2007, SO2_2008, SO2_2009, \n",
    "           SO2_2010, SO2_2011, SO2_2012, SO2_2013, SO2_2014, SO2_2015, SO2_2016, SO2_2017, SO2_2018, SO2_2019, \n",
    "           SO2_2020, SO2_2021]\n",
    "\n",
    "SO2 = pd.concat(SO2_all, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"font-size:1.2em;\"> <a id=\"Creating Trim Pollutant Datasets\">Creating Trim Pollutant Datasets</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T15:20:03.010891Z",
     "start_time": "2022-01-25T15:20:02.932212Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to trim all four pollutant datasets\n",
    "\n",
    "def trim_dataset(df, pollutant):\n",
    "    \"\"\"\n",
    "    Trims down pollution datasets.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    df: DataFrame\n",
    "    pollutant: str, name of pollutant\n",
    "    \n",
    "    Returns:\n",
    "    ----------\n",
    "    DataFrame\n",
    "    \"\"\"\n",
    "    \n",
    "    # Drop columns that are not US states or DC\n",
    "    df.drop(df[(df['State Name'] == 'Country Of Mexico') | \n",
    "               (df['State Name'] == 'Virgin Islands') | \n",
    "               (df['State Name'] == 'Canada') | \n",
    "               (df['State Name'] == 'Puerto Rico')].index, inplace=True)\n",
    "    \n",
    "    # Drop pollutant standards that do not produce AQI values\n",
    "    if pollutant == 'CO':\n",
    "        df.drop(df[df['Pollutant Standard'] == ('CO 1-hour 1971')].index, inplace=True)\n",
    "    elif pollutant == 'SO2':\n",
    "        df.drop(df[df['Pollutant Standard'] == ('SO2 3-hour 1971')].index, inplace=True)\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    # Drop columns that are redundant or unnecessary\n",
    "    df.drop(['State Code', 'County Code', 'Site Num', 'Parameter Code', 'POC', 'Latitude', 'Longitude', \n",
    "             'Datum', 'Parameter Name', 'Sample Duration', 'Pollutant Standard', 'Units of Measure', \n",
    "             'Event Type', 'Observation Count', 'Observation Percent', 'Method Code', 'Method Name', \n",
    "             'Local Site Name', 'Address', 'CBSA Name', 'Date of Last Change'], axis=1, inplace=True)\n",
    "    \n",
    "    # Reorder columns for neatness\n",
    "    reordered = ['Date Local', 'State Name', 'County Name', 'City Name', \n",
    "                 'Arithmetic Mean', '1st Max Value', '1st Max Hour', 'AQI']\n",
    "    \n",
    "    df = df.reindex(columns=reordered)\n",
    "    \n",
    "    # Rename columns for neatness\n",
    "    df = df.rename(columns={'Date Local': 'Date', \n",
    "                            'State Name': 'State', \n",
    "                            'County Name': 'County', \n",
    "                            'City Name': 'City', \n",
    "                            'Arithmetic Mean': 'Mean', \n",
    "                            'AQI': '{} AQI'.format(pollutant)})\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T15:21:09.981236Z",
     "start_time": "2022-01-25T15:20:03.014637Z"
    }
   },
   "outputs": [],
   "source": [
    "# Applying the function to datasets\n",
    "O3trim = trim_dataset(O3, 'O3')\n",
    "COtrim = trim_dataset(CO, 'CO')\n",
    "NO2trim = trim_dataset(NO2, 'NO2')\n",
    "SO2trim = trim_dataset(SO2, 'SO2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"font-size:1.2em;\"> <a id=\"Creating Time Series Datasets\">Creating Time Series Datasets</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T15:21:10.048839Z",
     "start_time": "2022-01-25T15:21:10.003289Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to make time series datasets\n",
    "\n",
    "def ts_dataset(df, pollutant):\n",
    "    \"\"\"\n",
    "    Creates time series datasets from trimmed pollutant datasets.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    df: DataFrame\n",
    "    pollutant: str, name of pollutant\n",
    "    \n",
    "    Returns:\n",
    "    ----------\n",
    "    DataFrame\n",
    "    \"\"\"\n",
    "    \n",
    "    # Drop mean, max value, and max hour columns\n",
    "    df.drop(['Mean', '1st Max Value', '1st Max Hour'], axis=1, inplace=True)\n",
    "    \n",
    "    # Set date as index\n",
    "    df.set_index(['Date'], inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T15:21:12.808527Z",
     "start_time": "2022-01-25T15:21:10.068995Z"
    }
   },
   "outputs": [],
   "source": [
    "# Applying the function to create time series datasets\n",
    "O3ts = ts_dataset(O3trim, 'O3')\n",
    "COts = ts_dataset(COtrim, 'CO')\n",
    "NO2ts = ts_dataset(NO2trim, 'NO2')\n",
    "SO2ts = ts_dataset(SO2trim, 'SO2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T15:21:12.838722Z",
     "start_time": "2022-01-25T15:21:12.811881Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to make NYC time series datasets\n",
    "\n",
    "def nyc_ts_dataset(df, pollutant):\n",
    "    \"\"\"\n",
    "    Creates NYC time series datasets from time series datasets.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    df: DataFrame\n",
    "    pollutant: str, name of pollutant\n",
    "    \n",
    "    Returns:\n",
    "    ----------\n",
    "    DataFrame\n",
    "    \"\"\"\n",
    "    \n",
    "    # Keep rows for NYC then drop location columns\n",
    "    df = df[df['City']=='New York']\n",
    "    df.drop(['State', 'County', 'City'], axis=1, inplace=True)\n",
    "    \n",
    "    # Groupby date and take max AQI values if there are duplicates of the same date\n",
    "    df = df.groupby('Date').max()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T15:21:14.056370Z",
     "start_time": "2022-01-25T15:21:12.855182Z"
    }
   },
   "outputs": [],
   "source": [
    "# Applying the function to create NYC time series datasets\n",
    "nycO3 = nyc_ts_dataset(O3ts, 'O3')\n",
    "nycCO = nyc_ts_dataset(COts, 'CO')\n",
    "nycNO2 = nyc_ts_dataset(NO2ts, 'NO2')\n",
    "nycSO2 = nyc_ts_dataset(SO2ts, 'SO2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"font-size:1.2em;\"> <a id=\"Export\">Export</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T15:21:14.069521Z",
     "start_time": "2022-01-25T15:21:14.059665Z"
    }
   },
   "outputs": [],
   "source": [
    "# Export individual pollutant datasets\n",
    "# O3.to_csv('O3.csv', index=False)\n",
    "# CO.to_csv('CO.csv', index=False)\n",
    "# NO2.to_csv('NO2.csv', index=False)\n",
    "# SO2.to_csv('SO2.csv', index=False)\n",
    "\n",
    "# Export trim pollutant datasets\n",
    "# O3.to_csv('O3trim.csv', index=False)\n",
    "# CO.to_csv('COtrim.csv', index=False)\n",
    "# NO2.to_csv('NO2trim.csv', index=False)\n",
    "# SO2.to_csv('SO2trim.csv', index=False)\n",
    "\n",
    "# Export time series datasets\n",
    "# O3ts.to_csv('O3ts.csv')\n",
    "# COts.to_csv('COts.csv')\n",
    "# NO2ts.to_csv('NO2ts.csv')\n",
    "# SO2ts.to_csv('SO2ts.csv')\n",
    "\n",
    "# Export NYC time series datasets\n",
    "# nycO3.to_csv('nycO3.csv')\n",
    "# nycCO.to_csv('nycCO.csv')\n",
    "# nycNO2.to_csv('nycNO2.csv')\n",
    "# nycSO2.to_csv('nycSO2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"font-size:1.2em;\"> <a id=\"Sources\">Sources</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [EPA AirData Daily Summary Data](https://aqs.epa.gov/aqsweb/airdata/download_files.html#Daily)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
